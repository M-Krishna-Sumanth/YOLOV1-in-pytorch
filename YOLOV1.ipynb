{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O_7IYjCMqWp",
    "outputId": "449124a1-27ae-466b-8ce6-31ae1066e7c9"
   },
   "outputs": [],
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "\n",
    "!tar xvf VOCtrainval_06-Nov-2007.tar\n",
    "!tar xvf VOCtest_06-Nov-2007.tar \n",
    "\n",
    "!rm VOCtrainval_06-Nov-2007.tar\n",
    "!rm VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHpQe56hfwgb"
   },
   "source": [
    "VOCdevkit/VOC2007/ has\n",
    "- <b>Annotations:-</b>It has .xml files that contain the description of the objects in the image. There is one xml file for each image.\n",
    "- <b>ImageSets</b>\n",
    "  - <b>Layout</b>\n",
    "    - test.txt\n",
    "\t- train.txt\n",
    "\t- trainval.txt\n",
    "\t- val.txt\n",
    "  - <b>Main:-</b>It has the following set of files for each class. These are for object detection task.\n",
    "    - horse_test.txt\n",
    "\t- horse_train.txt\n",
    "\t- horse_trainval.txt\n",
    "\t- horse_val.txt\n",
    "\t.\n",
    "\t.\n",
    "\t.\n",
    "\t- train.txt\n",
    "\t- test.txt\n",
    "\t- val.txt\n",
    "\t- trainval.txt\n",
    "  - <b>Segmentation:-</b>It has the following set of files. These are for semantic segmentation task\n",
    "    - test.txt\n",
    "    - train.txt\n",
    "\t- trainval.txt\n",
    "\t- val.txt\n",
    "- <b>JPEGImages:-</b>It has .jpg images\n",
    "- <b>SegmentationClass:-</b>These have .png images with class segmentation.\n",
    "- <b>SegmentationObject:-</b>These have .png images with object segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrZSCWgtfkGY"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import torchvision \n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0AM1IrjBFra"
   },
   "source": [
    "##Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y05J-087BCWO",
    "outputId": "0c2adf54-e921-4b3c-8803-54a3e6af0082"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z017beMgBJT"
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BENxNhKf_nI"
   },
   "outputs": [],
   "source": [
    "classes = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, \n",
    "           'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, \n",
    "           'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, \n",
    "           'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "\n",
    "for grp in ['train','test','val']:\n",
    "\n",
    "  rows = [['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cls_id']]\n",
    "    \n",
    "  with open(f'VOCdevkit/VOC2007/ImageSets/Main/{grp}.txt', 'r') as file:\n",
    "    img_ids = [img_id.strip() for img_id in file.read().strip().split('\\n')]\n",
    "    \n",
    "    for img_id in img_ids:\n",
    "    \n",
    "      filename = f'VOCdevkit/VOC2007/JPEGImages/{img_id}.jpg'\n",
    "      xml = f'VOCdevkit/VOC2007/Annotations/{img_id}.xml'\n",
    "      tree = ET.parse(xml)\n",
    "      root = tree.getroot()\n",
    "    \n",
    "      for obj in root.findall('object'):\n",
    "            \n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        keys = set(classes.keys())\n",
    "        if cls in keys and int(difficult) != 1:\n",
    "          cls_id = classes[cls]\n",
    "          box = obj.find('bndbox')\n",
    "          xmin = int(box.find('xmin').text)\n",
    "          ymin = int(box.find('ymin').text)\n",
    "          xmax = int(box.find('xmax').text)\n",
    "          ymax = int(box.find('ymax').text)\n",
    "          rows.append([filename, xmin, ymin, xmax, ymax, cls_id])\n",
    "        else:continue\n",
    "        \n",
    "  with open(f'VOCdevkit/VOC2007/{grp}.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNcaL9aIgEHI",
    "outputId": "9d649e52-408b-4ed9-a28b-815347172142"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('VOCdevkit/VOC2007/train.csv')\n",
    "train['cls_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLdW-wxlgGV6",
    "outputId": "5b407e4f-4d50-45c2-fb78-feecf29619b1"
   },
   "outputs": [],
   "source": [
    "val = pd.read_csv('VOCdevkit/VOC2007/val.csv')\n",
    "val ['cls_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rc8qwJVigKT6",
    "outputId": "2cb94be6-3cec-41fa-d62d-089fae2fc38d"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('VOCdevkit/VOC2007/test.csv')\n",
    "test['cls_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4LxxRwDOgNGu",
    "outputId": "c75c6196-079b-4375-9636-a37aefa6dd4a"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKPHfqghgcAN"
   },
   "source": [
    "## Prepare train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpfjbmy_gat_"
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "with open('VOCdevkit/VOC2007/train.csv', 'r') as file:\n",
    "  train_reader = csv.reader(file)\n",
    "  next(train_reader)\n",
    "  for line in train_reader:\n",
    "    x_train.append(line[0])\n",
    "    y_train.append([int(value) for value in line[1:]])\n",
    "\n",
    "with open('VOCdevkit/VOC2007/val.csv', 'r') as file:\n",
    "  val_reader = csv.reader(file)\n",
    "  next(val_reader)\n",
    "  for line in val_reader:\n",
    "    x_val.append(line[0])\n",
    "    y_val.append([int(value) for value in line[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0CRABHbgjpL"
   },
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fILmR_UQfeve"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset) :\n",
    "\n",
    "  def __init__(self, images, labels):    \n",
    "    self.images = images\n",
    "    self.labels = labels\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    img_path = self.images[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    image = torchvision.io.read_image(img_path)\n",
    "    height, width = image.shape[1:]\n",
    "    image = torchvision.transforms.ToPILImage()(image)\n",
    "    image = torchvision.transforms.Resize((448,448))(image)\n",
    "    image = torchvision.transforms.ToTensor()(image)\n",
    "    image = image/255\n",
    "    label_matrix = torch.zeros((30,7,7))\n",
    "  \n",
    "    x_min = label[0]\n",
    "    y_min = label[1]\n",
    "    x_max = label[2]\n",
    "    y_max = label[3]\n",
    "    clss = label[4]\n",
    "    x = (x_min+x_max)/2/width\n",
    "    y = (y_min+y_max)/2/height   \n",
    "    w = (x_max-x_min)/width\n",
    "    h = (y_max-y_min)/height\n",
    "    loc = [7*x, 7*y]\n",
    "    loc_y = int(loc[1])\n",
    "    loc_x = int(loc[0])\n",
    "    x = loc[0] - loc_x\n",
    "    y = loc[1] - loc_y\n",
    "  \n",
    "    # note:- these are matrix co-ordinates\n",
    "    label_matrix[20:25, loc_y, loc_x] = torch.tensor([1, x, y, w, h])\n",
    "    label_matrix[clss, loc_y, loc_x] = 1\n",
    "\n",
    "    return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBqfyXaIhE2D"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ymtc0qwhUb_"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-Z4OOOo3C4j"
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, **kwargs):\n",
    "    super(CNNBlock, self).__init__()\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "    self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.leakyrelu(self.conv(x))\n",
    "\n",
    "class YoloV1(nn.Module):\n",
    "  def __init__(self, architecture):\n",
    "    super(YoloV1, self).__init__()\n",
    "    self.darknet = self._create_layers(architecture)\n",
    "      \n",
    "  def forward(self, x):\n",
    "    return self.darknet(x)\n",
    "      \n",
    "  def _create_layers(self, architecture, in_channels=3):\n",
    "    layers = []\n",
    "    #architecture should have two parts. First part should contain conv layers and second part should contain linear layers.\n",
    "    for x in architecture[0]:\n",
    "      if type(x) == tuple:\n",
    "        layers.append(CNNBlock(in_channels, out_channels=x[1], kernel_size=x[0], stride=x[2], padding=x[0]//2))\n",
    "        in_channels = x[1]\n",
    "      elif type(x) == str:\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      elif type(x) == list:\n",
    "        for y in range(x[-1]):\n",
    "          for z in x[:-1]:\n",
    "            layers.append(CNNBlock(in_channels, out_channels=z[1], kernel_size=z[0], stride=z[2], padding=z[0]//2))\n",
    "            in_channels = z[1]\n",
    "    \n",
    "    layers.append(nn.Flatten())\n",
    "    in_features = architecture[1][0]\n",
    "    for x in architecture[1][1:]:\n",
    "      layers.append(nn.Linear(in_features, x))\n",
    "      in_features = x\n",
    "      \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joeIB0fZhkrW"
   },
   "source": [
    "###Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf0GQF9Hhi68"
   },
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "\n",
    "  def __init__(self, S=7, B=2, C=20):\n",
    "    super(YoloLoss, self).__init__()\n",
    "    self.S = S\n",
    "    self.B = B\n",
    "    self.C = C\n",
    "    self.lambda_coord = 0.5\n",
    "    self.lambda_noobj = 5\n",
    "  \n",
    "  #pred and true are (m,30,s,s) tensors\n",
    "  def forward(self, pred, true):\n",
    "    pred = torch.reshape(pred,(-1, self.B*5+self.C, self.S, self.S))\n",
    "    true = torch.reshape(true,(-1, self.B*5+self.C, self.S, self.S))\n",
    "    iou_b1 = IOU(yolo_to_pixels(pred[:,21:25,:,:]), yolo_to_pixels(true[:,21:25,:,:]))\n",
    "    iou_b2 = IOU(yolo_to_pixels(pred[:,26:30,:,:]), yolo_to_pixels( true[:,21:25,:,:]))\n",
    "    ious = torch.cat([iou_b1, iou_b2], dim=1)                         #ious is an (m,2,s,s) tensor\n",
    "    iou_maxes, best_box = torch.max(ious, dim=1, keepdim=True)        #iou_maxes and best_box are (m,1,s,s) tensors\n",
    "    exists_box = true[:,20:21,:,:]                                    #exists_box is an (m,1,s,s) tensor\n",
    "    \n",
    "    #box loss\n",
    "    #box_true and box_pred are (m, 4, s, s) tensors\n",
    "    box_pred = exists_box * (best_box*pred[:,26:30,:,:]+(1-best_box)*pred[:,21:25,:,:])\n",
    "    box_pred[:,2:,:,:] = torch.sign(box_pred[:,2:,:,:])*torch.sqrt(torch.abs(box_pred[:,2:,:,:]+1e-6))\n",
    "    box_true = exists_box * true[:,21:25,:,:]\n",
    "    box_true[:,2:,:,:] = torch.sqrt(box_true[:,2:,:,:])\n",
    "    diff_box = box_true-box_pred\n",
    "    box_loss = torch.sum(torch.square(diff_box))\n",
    "    \n",
    "    #object loss\n",
    "    obj_pred = exists_box*(best_box*pred[:,25:26,:,:]+(1-best_box)*pred[:,20:21,:,:])\n",
    "    obj_true = exists_box * true[:,20:21,:,:]\n",
    "    diff_obj = obj_true-obj_pred\n",
    "    obj_loss = torch.sum(torch.square(diff_obj))\n",
    "    \n",
    "    #no object loss\n",
    "    noobj_pred = (1-exists_box) * ((pred[:,25:26,:,:]+pred[:,20:21,:,:])/2)\n",
    "    noobj_true = (1-exists_box) * true[:,20:21,:,:]\n",
    "    diff_noobj = noobj_true-noobj_pred\n",
    "    noobj_loss = torch.sum(torch.square(diff_noobj))\n",
    "    \n",
    "    #class loss\n",
    "    class_pred = exists_box * pred[:,:20,:,:]\n",
    "    class_true = exists_box * true[:,:20,:,:]\n",
    "    diff_class = class_true-class_pred\n",
    "    class_loss = torch.sum(torch.square(diff_class))\n",
    "    \n",
    "    #total loss\n",
    "    loss = self.lambda_coord*box_loss + obj_loss + self.lambda_noobj*noobj_loss + class_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HApWS2hJh2ve"
   },
   "source": [
    "###Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLzl2O5UhqGN"
   },
   "outputs": [],
   "source": [
    "def yolo_to_pixels(boxes, S=7, N=448):\n",
    "  new_boxes = boxes.clone().detach()\n",
    "  x_ind_tensor = torch.reshape(torch.arange(0,7),(1,1,1,7)).to(device)\n",
    "  y_ind_tensor = torch.reshape(torch.arange(0,7),(1,1,7,1)).to(device)\n",
    "  new_boxes[:,0:1,:,:] = (new_boxes[:,0:1,:,:]!=0)*(x_ind_tensor+new_boxes[:,0:1,:,:])*(N/S)\n",
    "  new_boxes[:,1:2,:,:] = (new_boxes[:,1:2,:,:]!=0)*(y_ind_tensor+new_boxes[:,1:2,:,:])*(N/S)\n",
    "  new_boxes[:,2:4,:,:] = new_boxes[:,2:4,:,:]*N\n",
    "  return new_boxes\n",
    "\n",
    "def pixels_to_yolo(img_paths, labels):\n",
    "  yolo_images = []\n",
    "  yolo_labels = []\n",
    "  for img_path, label in zip(img_paths, labels):\n",
    "    image = torchvision.io.read_image(img_path)\n",
    "    height, width = image.shape[1:]\n",
    "    image = torchvision.transforms.ToPILImage()(image)\n",
    "    image = torchvision.transforms.Resize((448,448))(image)\n",
    "    image = torchvision.transforms.ToTensor()(image)\n",
    "    image = image/255\n",
    "    label_matrix = torch.zeros((30,7,7))\n",
    "    \n",
    "    x_min = label[0]\n",
    "    y_min = label[1]\n",
    "    x_max = label[2]\n",
    "    y_max = label[3]\n",
    "    clss = label[4]\n",
    "    x = (x_min+x_max)/2/width\n",
    "    y = (y_min+y_max)/2/height   \n",
    "    w = (x_max-x_min)/width\n",
    "    h = (y_max-y_min)/height\n",
    "    loc = [7*x, 7*y]\n",
    "    loc_y = int(loc[1])\n",
    "    loc_x = int(loc[0])\n",
    "    x = loc[0] - loc_x\n",
    "    y = loc[1] - loc_y\n",
    "    \n",
    "    # note:- these are matrix co-ordinates\n",
    "    label_matrix[ 20:25, loc_y, loc_x] = torch.tensor([1, x, y, w, h])\n",
    "    label_matrix[ clss, loc_y, loc_x] = 1\n",
    "    \n",
    "    yolo_images.append(image)\n",
    "    yolo_labels.append(label_matrix)\n",
    "  \n",
    "  return torch.stack(yolo_images), torch.stack(yolo_labels)\n",
    "#pred, true are of shape (m,4,s,s) with format [x_mid, y_mid, width, height]\n",
    "def IOU(pred, true, S=7):\n",
    "  #following are of shape (m,1,s,s)\n",
    "  x_pred = pred[:,0:1,:,:]\n",
    "  y_pred = pred[:,1:2,:,:]\n",
    "  w_pred = pred[:,2:3,:,:]\n",
    "  h_pred = pred[:,3:4,:,:]\n",
    "  x_true = true[:,0:1,:,:]\n",
    "  y_true = true[:,1:2,:,:]\n",
    "  w_true = true[:,2:3,:,:]\n",
    "  h_true = true[:,3:4,:,:]\n",
    "  x_max = torch.max(x_pred-w_pred/2, x_true-w_true/2)\n",
    "  y_max = torch.max(y_pred-h_pred/2, y_true-h_true/2)\n",
    "  x_min = torch.min(x_pred+w_pred/2, x_true+w_true/2)\n",
    "  y_min = torch.min(y_pred+h_pred/2, y_true+h_true/2)\n",
    "  \n",
    "  intersection = (x_min-x_max) * (y_min-y_max)\n",
    "  union = w_pred * h_pred + w_true * h_true - intersection\n",
    "  \n",
    "  iou = intersection/union\n",
    "  iou[iou!=iou] = 0\n",
    "\n",
    "  return iou\n",
    "\n",
    "def yolo_to_boxes(out, S=7, box_thre=0.9):\n",
    "  #out is of shape (m,30,7,7)\n",
    "  #boxes1, boxes2, boxes are of shape (m,4,7,7)\n",
    "  boxes1 = out[:,21:25,:,:]\n",
    "  boxes2 = out[:,26:30,:,:]\n",
    "  prob_scores = torch.cat([out[:,20:21,:,:],out[:,25:26,:,:]],dim=1) #prob_scores is of shape (m,2,7,7)\n",
    "  best_prob, best_box = torch.max(prob_scores, dim=1, keepdim=True)  #best_prob, best_box are of shape (m,1,7,7)\n",
    "  boxes = boxes1*(1-best_box)+best_box*boxes2\n",
    "  #clss is of shape (m,1,7,7)\n",
    "  clss = torch.argmax(out[:,:20,:,:],dim=1,keepdim=True)\n",
    "  bboxes = torch.cat([clss, best_prob, yolo_to_pixels(boxes)], dim=1)#bboxes is of shape (m,6,7,7)\n",
    "  bboxes = torch.reshape(bboxes, (-1, 6, S*S))\n",
    "  new_bboxes = {}\n",
    "  \n",
    "  for i in range(bboxes.shape[0]):\n",
    "    new_bboxes[i]=[]\n",
    "    for cell in range(bboxes.shape[-1]):\n",
    "      temp=[]\n",
    "      if bboxes[i,:,cell][1]>box_thre:\n",
    "        for value in bboxes[i,:,cell]:\n",
    "          temp.append(round(float(value),4))\n",
    "        new_bboxes[i].append(temp)\n",
    "\n",
    "  return new_bboxes\n",
    "\n",
    "def iou(pred, true, S=7):\n",
    "  #following are of shape (m,1,s,s)\n",
    "  x_pred = pred[0]\n",
    "  y_pred = pred[1]\n",
    "  w_pred = pred[2]\n",
    "  h_pred = pred[3]\n",
    "  x_true = true[0]\n",
    "  y_true = true[1]\n",
    "  w_true = true[2]\n",
    "  h_true = true[3]\n",
    "  x_max = torch.max(x_pred-w_pred/2, x_true-w_true/2)\n",
    "  y_max = torch.max(y_pred-h_pred/2, y_true-h_true/2)\n",
    "  x_min = torch.min(x_pred+w_pred/2, x_true+w_true/2)\n",
    "  y_min = torch.min(y_pred+h_pred/2, y_true+h_true/2)\n",
    "  \n",
    "  intersection = (x_min-x_max) * (y_min-y_max)\n",
    "  union = w_pred * h_pred + w_true * h_true - intersection\n",
    "  \n",
    "  iou = intersection/union\n",
    "  iou[iou!=iou] = 0\n",
    "\n",
    "  return iou\n",
    "\n",
    "#pred is of format {0:[[class, box_prob, x_mid, y_mid, width, height],...],...}\n",
    "def NMS(pred, iou_thre=0.5):\n",
    "  for key in pred:\n",
    "    pred[key] = sorted(pred[key], key=lambda x:x[1], reverse=True)\n",
    "  nms_boxes = {}\n",
    "  \n",
    "  for key in pred:\n",
    "    nms_boxes[key] = []\n",
    "    while pred[key]:\n",
    "      chosen_box = pred[key].pop(0)\n",
    "      pred[key] = [box \n",
    "                   for box in pred[key] \n",
    "                   if box[0]!=chosen_box[0] or\n",
    "                   iou(torch.tensor(box),torch.tensor(chosen_box))<iou_thre]\n",
    "      nms_boxes[key].append(chosen_box)\n",
    "\n",
    "  return nms_boxes\n",
    "\n",
    "def plot_image(boxes, img_path):\n",
    "  size = 448\n",
    "  image = cv.imread(img_path)\n",
    "  image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "  image = cv.resize(image,(448,448))\n",
    "  for box in boxes:\n",
    "    cls = int(box[0])\n",
    "    x = box[2]\n",
    "    y = box[3]\n",
    "    w = box[4]\n",
    "    h = box[5]\n",
    "    x_min = int(max(x-w/2,0))\n",
    "    y_min = int(max(y-h/2,10))\n",
    "    x_max = int(min(x+w/2,448))\n",
    "    y_max = int(min(y+h/2,448))\n",
    "    cv.rectangle(image,(x_min,y_min),(x_max,y_max),(255,255,255),2)\n",
    "    cv.putText(image, list(classes.keys())[list(classes.values()).index(cls)], \\\n",
    "            (x_min, y_min-10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "    plt.imshow(image) \n",
    "\n",
    "def MAP(pred, true, n_classes, iou_thresh=0.5):\n",
    "  #pred, true are of format [idx, class, obj_prob, x1, y1, x2, y2]\n",
    "  avg_prec = []\n",
    "    \n",
    "  for c in range(n_classes):\n",
    "    predictions = []\n",
    "    truths = []\n",
    "        \n",
    "    for prediction in pred:\n",
    "      if prediction[1]==c:\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "      for truth in true:\n",
    "        if truth[1]==c:\n",
    "          truths.append(truth)\n",
    "        \n",
    "    boxes_per_img = Counter([truth[0] for truth in truths])\n",
    "        \n",
    "    for key, val in boxes_per_img.items():\n",
    "      boxes_per_img[key] = torch.zeros(val)\n",
    "            \n",
    "      predictions.sort(key=lambda x:x[2], reverse=True)\n",
    "      TP = torch.zeros(len(predictions))\n",
    "      FP = torch.zeros(len(predictions))\n",
    "      n_truths  = len(truths)\n",
    "        \n",
    "      for pred_idx, prediction in enumerate(predictions):\n",
    "        truth_img = [truth for truth in truths if truth[0]==prediction[0]]\n",
    "        n_truths_img = len(truth_img)\n",
    "        best_iou = 0\n",
    "            \n",
    "        for idx, truth in enumerate(truth_img):\n",
    "          iou = IOU(prediction[3:], truth[3:])\n",
    "          if iou>best_iou:\n",
    "            best_iou = iou\n",
    "            best_truth_idx = idx\n",
    "            \n",
    "          if best_iou>iou_thresh:\n",
    "            if boxes_per_img[prediction[0]][best_truth_idx]==0:\n",
    "              TP[pred_idx]=1\n",
    "              boxes_per_img[prediction[0]][best_truth_idx]=1\n",
    "            else:\n",
    "              FP[pred_idx]=1\n",
    "          else:\n",
    "            FP[pred_idx]=1\n",
    "            \n",
    "      TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "      FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "      recall = TP_cumsum/n_truths\n",
    "      precision = TP_cumsum/(TP_cumsum+FP_cumsum)\n",
    "      precision = torch.cat(torch.tensor([1]), precision)\n",
    "      recall = torch.cat(torch.tensor([0]), recall)\n",
    "      avg_prec.append(torch.trapz(precision,recall))\n",
    "        \n",
    "    return sum(avg_prec)/len(avg_prec)\n",
    "\n",
    "def save_chkpt(model, optim, filename='drive/MyDrive/chkpt.pth.tar'):\n",
    "  chkpt = {'state_dict':model.state_dict(),'optimizer':optim.state_dict()}\n",
    "  torch.save(chkpt, filename)\n",
    "\n",
    "def load_chkpt(model, optim, filename='drive/MyDrive/chkpt.pth.tar'):\n",
    "  chkpt = torch.load(filename)\n",
    "  model.load_state_dict(chkpt['state_dict'])\n",
    "  optim.load_state_dict(chkpt['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1FAwhxm0hbu"
   },
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB830t_2BgQk"
   },
   "source": [
    "###Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJVmxn6-7qND"
   },
   "outputs": [],
   "source": [
    "C = 20\n",
    "S = 7\n",
    "B = 2\n",
    "N = 448\n",
    "\n",
    "architecture = [[\n",
    "#tuple : (kernel_size, n_filters, stride)\n",
    "(7,64,2),\n",
    "#M : maxpooling layer with kernel_size=2 and stride=2\n",
    "'M',\n",
    "(3,192,1),\n",
    "'M',\n",
    "(1,128,1),\n",
    "(3,256,1),\n",
    "(1,256,1),\n",
    "(3,512,1),\n",
    "'M',\n",
    "#list : [tuples, repeat]\n",
    "[(1,256,1),(3,512,1),4],\n",
    "(1,512,1),\n",
    "(3,1024,1),\n",
    "'M',\n",
    "[(1,512,1),(3,1024,1),2],\n",
    "(3,1024,1),\n",
    "(3,1024,2),\n",
    "(3,1024,1),\n",
    "(3,1024,1)],\n",
    "[\n",
    "# int : n_neurons\n",
    "7*7*1024,\n",
    "4096,\n",
    "S*S*(C+B*5)\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlFdteF30602"
   },
   "outputs": [],
   "source": [
    "load_model = True\n",
    "model = YoloV1(architecture).to(device)\n",
    "yololoss = YoloLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "if load_model:\n",
    "  load_chkpt(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqNWtWTGRSBl",
    "outputId": "8d4ea71c-7cda-4736-8ebd-2ffc609b9fc4"
   },
   "outputs": [],
   "source": [
    "summary(model,(3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7zsxFqnwyew"
   },
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "def fit(x_train, y_train, model, optimizer, epochs):\n",
    "  train_set = Dataset(x_train, y_train)\n",
    "  train_loader = DataLoader(train_set, 16, True)\n",
    "  for epoch in range(epochs):\n",
    "    batch_losses = []\n",
    "    if epoch%1==0 and epoch!=0:\n",
    "      save_chkpt(model, optimizer)\n",
    "    loop = tqdm(train_loader,  position=0, leave=True)\n",
    "    for x, y in loop:\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = yololoss(y_hat, y)\n",
    "      batch_losses.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loop.set_postfix(loss=loss)\n",
    "\n",
    "    epoch_losses.append(sum(batch_losses)/len(batch_losses))\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTkB-xNtN7q-"
   },
   "outputs": [],
   "source": [
    "def predict(x_test, model):\n",
    "  images = []\n",
    "  for img_path in x_test:\n",
    "    image = torchvision.io.read_image(img_path)\n",
    "    height, width = image.shape[1:]\n",
    "    image = torchvision.transforms.ToPILImage()(image)\n",
    "    image = torchvision.transforms.Resize((448,448))(image)\n",
    "    image = torchvision.transforms.ToTensor()(image)\n",
    "    image = image/255\n",
    "    images.append(image)\n",
    "  \n",
    "  y_hat = model(torch.stack(images))\n",
    "  y_hat = torch.reshape(y_hat,(len(x_test),30,7,7))\n",
    " \n",
    "  boxes = NMS(yolo_to_boxes(y_hat))\n",
    "  return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmtCHve4ODJP",
    "outputId": "5a6243fd-4cb2-4e42-ef4f-e06e8e066bfb"
   },
   "outputs": [],
   "source": [
    "fit(x_train,y_train,model,optimizer,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTbviuHHwLJ5"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "x_test = ['VOCdevkit/VOC2007/JPEGImages/000017.jpg']\n",
    "boxes = predict(x_test,model)\n",
    "plot_image(boxes[0],x_test[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
